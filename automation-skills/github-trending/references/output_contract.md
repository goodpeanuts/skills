# GitHub Trending Report Output Contract

This document defines machine-checkable rules for report outputs.

## 1. Required Files

For the workflow-selected report directory, all files must exist:
1. `original_trending.html`
2. `report_<YYYY-MM-DD>.md`
3. `report_<YYYY-MM-DD>.html`
4. `report_manifest.json`

## 2. Path Contract

1. `<period>` must be one of: `daily`, `weekly`, `monthly`.
2. `<YYYY-MM-DD>` must match the date used in report filenames and metadata.
3. Filenames must use the exact date from the directory name.
4. Output root policy is defined in `SKILL.md` (single source of truth).

## 3. Source Completeness Contract

1. `original_trending.html` is the source of truth.
2. `source_item_count` in `report_manifest.json` must equal extracted source repo count.
3. No fixed minimum source count is required; strict requirement is full correspondence with source list (same repos, same order, no omissions).

## 4. Unified Field Quality Contract

All output formats (Markdown/HTML) must include these fields with the following unified requirements:
These are semantic requirements and are not fully machine-gated by `scripts/validate_report.py`.

### æ˜¯ä»€ä¹ˆ
- **Structure**: 1-2 sentence description
- **Quality**: Concise, accurate repo summary

### ä½œç”¨
- **Structure**: What problem it solves, for whom
- **Quality**: Clear value proposition and target audience

### æ•ˆæœ
- **Structure**: Attention signal + specific value
- **Quality Requirements**:
  - Must include observable attention/adoption signal (stars/forks/activity/momentum)
  - Must explain specific value for how to benefit daily work and improve productivity.
  - Template: "å…³æ³¨åº¦[ä¿¡å·]ï¼Œå¯¹æˆ‘[å…·ä½“å·¥ä½œåœºæ™¯]çš„ä»·å€¼æ˜¯[å…·ä½“å¸®åŠ©]"
  - Good: "å…³æ³¨åº¦æŒç»­ä¸Šå‡(starså‘¨å¢500+)ï¼Œå¯¹æˆ‘çš„Agent PoCé¡¹ç›®çš„ä»·å€¼æ˜¯æä¾›äº†ä¸€å¥—å¯ç›´æ¥å¤ç”¨çš„æ²™ç®±éš”ç¦»æ–¹æ¡ˆ"
  - Bad: "å…³æ³¨åº¦è¾ƒé«˜ï¼Œå€¼å¾—å…³æ³¨"

### é¡¹ç›®åˆ†æ
- **Structure**: Tech stack + mechanism + evidence + relevance + tradeoffs
- **Quality Requirements**:
  - Must include: technical stack, evidence-based mechanism analysis, risk/tradeoff
  - Deep dive required when README insufficient or claims are non-trivial (performance/security/finance-critical)
  - Template: "æŠ€æœ¯æ ˆ[XX]ï¼Œæ ¸å¿ƒæœºåˆ¶[XX]ï¼Œè€ƒè™‘åˆ°æˆ‘çš„[é¡¹ç›®/éœ€æ±‚]ï¼Œ[å…·ä½“å¸®åŠ©ç‚¹]ï¼Œä½†éœ€æ³¨æ„[é£é™©/é™åˆ¶]"
  - Good: "æŠ€æœ¯æ ˆPython + async runtimeï¼Œæ ¸å¿ƒæ˜¯äº‹ä»¶é©±åŠ¨çš„ä»»åŠ¡è°ƒåº¦å™¨ã€‚è€ƒè™‘åˆ°æˆ‘æ­£åœ¨åšçš„å¤šAgentååŒé¡¹ç›®ï¼Œå®ƒçš„å¼‚æ­¥ç¼–æ’æœºåˆ¶å¯ä»¥ç›´æ¥è§£å†³å½“å‰ä¸²è¡Œæ‰§è¡Œçš„æ•ˆç‡é—®é¢˜ï¼Œä½†éœ€æ³¨æ„å…¶ä¾èµ–Python 3.11+ï¼Œä¸ç°æœ‰3.9ç¯å¢ƒä¸å…¼å®¹"
  - Bad: "æŠ€æœ¯æ ˆæ¸…æ™°ï¼Œæ¶æ„è®¾è®¡åˆç†ï¼Œé€‚åˆæ·±å…¥å­¦ä¹ "

### å»ºè®®
- **Structure**: Specific action + scope + prerequisite + expected outcome
- **Quality Requirements**:
  - **Required 4 elements**:
    1. Specific action (what exactly to do)
    2. Scope/boundary (where it applies, what's excluded)
    3. Prerequisite (what's needed before starting)
    4. Expected outcome (concrete benefit or metric)
  - **Forbidden phrases**: "å»ºè®®å…³æ³¨", "å€¼å¾—ç ”ç©¶", "å¯ä»¥å°è¯•", "å¯¹æå‡XXèƒ½åŠ›æœ‰å¸®åŠ©", "å»ºè®®æ·±å…¥å­¦ä¹ ", "å»ºè®®è¿›ä¸€æ­¥ç ”ç©¶"
  - Good: "ä¸‹å‘¨åœ¨Agent PoCä¸­å¼•å…¥è¯¥é¡¹ç›®çš„æ²™ç®±æ¨¡å—ï¼Œæ›¿æ¢å½“å‰ç®€å•çš„subprocesséš”ç¦»ã€‚é¢„è®¡2å¤©å®Œæˆé›†æˆï¼Œå¯æå‡å·¥å…·è°ƒç”¨çš„å®‰å…¨æ€§ï¼Œä½†éœ€å…ˆéªŒè¯å…¶å¯¹Windowsç¯å¢ƒçš„æ”¯æŒ"
  - Bad: "å»ºè®®æ·±å…¥å­¦ä¹ è¯¥é¡¹ç›®ï¼Œå¯¹æå‡Agentå¼€å‘èƒ½åŠ›æœ‰å¸®åŠ©"

### æ¦‚è¿°ä¸è¶‹åŠ¿åˆ†æ - å»ºè®®è¡ŒåŠ¨
- **Structure**: Overview section with prioritized action items
- **Quality Requirements**:
  - Must reference at least 2 specific repos from current report by name
  - Each action must include feasibility assessment (effort/risk/timeline)
  - Forbidden: generic advice applicable to any week
  - Good: "1. æœ¬å‘¨ä¼˜å…ˆè¯„ä¼° [microsoft/semantic-kernel] çš„æ’ä»¶ç³»ç»Ÿï¼Œå¯ä»¥è§£å†³å½“å‰Agenté¡¹ç›®ä¸­å·¥å…·æ³¨å†Œæ··ä¹±çš„é—®é¢˜ï¼Œä½†éœ€æ³¨æ„å…¶ä¸LangChainçš„ç”Ÿæ€å·®å¼‚"
  - Bad: "1. æ¯å‘¨ä¼˜å…ˆå®¡æŸ¥ Top é¡¹ç›®çš„ stars/forks å¢é€Ÿ"

## 5. Markdown Structure Contract

`report_<date>.md` must include:
1. Top-level title matching date
2. `## ğŸ“Š æ¦‚è¿°ä¸è¶‹åŠ¿åˆ†æ`
3. `## ğŸš€ çƒ­é—¨é¡¹ç›®è¯¦ç»†åˆ†æ`
4. Repo sections with exact pattern: `### N. [owner/repo](https://github.com/owner/repo)`
5. Sequential numbering from `1..N`

Each repo section must include all fields defined in Section 4 (Unified Field Quality Contract).

## 6. HTML Structure Contract

`report_<date>.html` must include:
1. `.overview-section`
2. `.repo-card` per repo
3. `.tag` badges (at least one per card)
4. `.suggestion-box` per card
5. Section heading `ğŸš€ çƒ­é—¨é¡¹ç›®è¯¦ç»†åˆ†æ`

Each card must include all fields defined in Section 4 (Unified Field Quality Contract).

Disallowed:
- Markdown backticks in HTML body text
- Invalid HTML structure patterns (`<p><ul>`, `<p><ol>`)

## 7. Manifest Contract

`report_manifest.json` must include:
1. `date` (string, `YYYY-MM-DD`)
2. `period` (`daily|weekly|monthly`)
3. `source_item_count` (integer)
4. `reported_item_count` (integer)
5. `repos` (ordered list)

Each `repos[]` item must include:
1. `rank` (1-based sequential integer)
2. `repo` (`owner/repo`)
3. `url` (`https://github.com/owner/repo`)

## 8. Cross-File Consistency

All counts must match exactly:
1. source list count from `original_trending.html`
2. markdown repo section count
3. html `.repo-card` count
4. `reported_item_count`
5. `len(repos)` in manifest

Any mismatch is a hard failure.

## 9. Source-to-Report Repo Identity (Critical)

Use `original_trending.html` as canonical source list.

For every generated report, these three repo sequences must exactly match source list:
1. Markdown repo sequence
2. HTML repo-card repo sequence
3. Manifest `repos[].repo` sequence

Exact match means:
1. Same length
2. Same repo names
3. Same order

If any repo is missing, replaced, or re-ordered, validation must fail.

## 10. Example Interpretation (Critical)

`references/example_report.md` and `references/example_report.html` are demonstration files for output format, section structure, and field ordering.
They are not depth benchmarks.
Generated reports must be materially deeper than example wording, especially in `æ•ˆæœ`, `é¡¹ç›®åˆ†æ`, and `å»ºè®®`.

## 11. Personalization Context

**Quality Markers**:
- Context-aware benefit analysis (not generic praise)
- Specific use case or project reference
- Risk/benefit tradeoff analysis when applicable
- Time-bounded or scope-limited action items

All field-level personalization requirements are defined in Section 4 (Unified Field Quality Contract).

## 12. Input & Path Contract

Input and path rules are defined in `SKILL.md` (single source of truth) to avoid duplicated definitions.

## 13. Evidence Collection Requirements

### Baseline Evidence Collection (every repo):
1. Read README and other essential files (e.g. setup.py, main source files)
2. Inspect repo structure (root + key directories/files)
3. Extract architecture, usage, and target-user signals
4. Collect attention signals (stars/forks/activity/momentum)
5. Identify primary technical stack (languages, frameworks, deployment pattern)

### Deep Dive Triggers:
When README/structure is insufficient OR claims are non-trivial (performance/security/finance-critical):
- Inspect at least 2 key code/module evidence points
- Reflect mechanism, risks, and tradeoffs in é¡¹ç›®åˆ†æ

## 14. Parallel Analysis Process

### Sub-Agent Spawning:
1. Spawn parallel sub-agents for each repository in Step 2 using Task tool (subagent_type: "general-purpose")
2. Launch ALL repo analysis sub-agents in a SINGLE message with multiple Task tool calls to maximize parallelism
3. This is a workflow semantic requirement and is not enforced by `scripts/validate_report.py`

### Sub-Agent Prompt Requirements:
Sub-agent prompts must reference Section 11 (Personalization) and Section 13 (Evidence Collection) for analysis requirements. Each sub-agent must:
1. Use `mcp__github__get_file_contents` to read README.md and key source files
2. Inspect repo structure to identify technical stack and architecture
3. Extract attention signals from repo metadata
4. Generate analysis following the personalization requirements in Section 11
5. Follow evidence collection guidelines in Section 13

### Aggregation:
After all sub-agents complete:
1. Assemble outputs into final report maintaining original ranking order
2. Synthesize æ¦‚è¿°ä¸è¶‹åŠ¿åˆ†æ section based on patterns across all repos
3. Ensure overview å»ºè®®è¡ŒåŠ¨ references specific repos from current report

### Error Handling:
If a sub-agent fails, retry that specific repo once. If still failing, log error and continue with remaining repos.

## 15. Retry & Failure Policy

### Validation Retry Policy:
1. If validation passes: continue to email send step
2. If validation fails: regenerate and validate again
3. Maximum retries: 2
4. If still failing after retries: stop and do not send email

### Hard Failure Conditions:
Stop and return errors if any of the following occurs:
1. Missing required output files
2. Source item mismatch with Markdown/HTML/manifest
3. Non-sequential ranking
4. Missing required analysis fields
5. Invalid GitHub repository links
6. Output path/date/period mismatch

Never send email when validation fails.
